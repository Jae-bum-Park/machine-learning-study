{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB 리뷰 data set\n",
    "- 리뷰를 감상평에따라 긍정과 부정으로 분류한 data set.\n",
    "- 자연어 처리(NLP) : 컴퓨터를 사용해 인간의 언어를 처리하는 분야.(ex> 음성인식, 기계번역, 감성분석.)\n",
    "- 텍스트 데이터를 숫자데이터로 바꾸는 일반적인 방법은. 단어마다 고유한 정수를 부여하는 것이다.\n",
    "- 일반적으로 영어문장은 소문자로 바꾸고 구둣점들을 삭제하고, 공백을 기준으로 분리한다.\n",
    "- 분리된 단어 한개를 토큰이라고 한다. 1개의 토큰은 한개의 타임스템프에 해당한다.\n",
    "- 정수중에 몇개는 특정한 용도로 예약되어있다.(ex> 0 : 패딩, 1 : 문장의시작, 2 : 어휘사전에 없는 토큰)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB data set 불러오기.\n",
    "- tensorflow의 imdb 데이터는 이미 정수로 변환되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# num_words=500 : 어휘사전에 500개의 단어만 들어가 있다. 사전에 없는 경우 모두 2로 표시한다.\n",
    "from tensorflow.keras.datasets import imdb\n",
    "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "#imdb 데이터는 numpy 1차원 배열로 되어있다.\n",
    "#numpy배열은 정수나 실수외에도 파이썬 객체를 담을 수 있다.\n",
    "print(len(train_input[0])) #첫번째 리뷰의 길이는 218개의 토큰으로 이루어져 있다.\n",
    "print(len(train_input[1])) #두번째 리뷰의 길이는 189개의 토큰으로 이루어져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#target값이 0(부정), 1(긍정)으로 나누어진다.\n",
    "print(train_target[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set, validation set 분류\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰의 평균길이와, 최소길이, 최장길이를 확인하기 위해, 각 리뷰의 길이를 계산해 numpy 배열에 담는다. \n",
    "import numpy as np\n",
    "lengths = np.array([len(x) for x in train_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.00925 178.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lengths), np.median(lengths)) #리뷰 길이의 평균과 중간값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#위의 값을 보았을때 한쪽에 치우친 분포를 알 수 있다.\n",
    "#히스토그램으로 그려보자.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리뷰 data의 길이를 맞추기 위해 패딩이 필요하다.\n",
    "#kearas의 sequence data의 길이를 맞추는 pad_sequences() 함수를 사용한다.\n",
    "#train_input의 길이를 100으로 맞춰본다.\n",
    "#maxlend에 원하는 길이를 지정하고 이보다 긴 경우는 잘라내고 짧은 경우는 0으로 패딩한다.\n",
    "#pad_sequence 함수는 기본적으로 앞부분을 자른다. 왜냐하면 뒷부분의 정보가 더 유용하다고 기대하기 때문이다.\n",
    "#뒷부분을 잘라내고 싶다면 truncating 매개변수의 기본값을 'pre'에서 'post'로 변경해주면 된다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_seq = pad_sequences(train_input, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100)\n"
     ]
    }
   ],
   "source": [
    "#길이를 100으로 맞춘 2차원 배열이 되었다.\n",
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증 set의 길이도 100으로 맞춰준다.\n",
    "val_seq = pad_sequences(val_input, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망 만들기.\n",
    "- IMDB는 이진분류이므로 마지막 출력은 시그모이드 함수를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어휘사전을 처음에 500개의 단어를 저장해 주었다.\n",
    "#그리고 한 리뷰에는 100개의 단어가 포함된다.\n",
    "#따라서 단어 한개를 고유한 정수로 표현하기 위해서는 원핫인코딩이 필요하고, 한 단어당 500개의 배열이 필요하다.\n",
    "#즉 토큰마다 500개의 숫자를 사용해 표현한다. 한개만 1이고 나머지는 0으로 만든다.\n",
    "from tensorflow import keras\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.SimpleRNN(8, input_shape=(100,500)))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##하지만 keras는 원-핫 인코딩을 위한 유틸리티를 제공한다.(*따로 수동으로 만들어줄 필요가 없다.)\n",
    "#keras.utils의 to_categorical()함수를 사용한다.\n",
    "train_oh = keras.utils.to_categorical(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100, 500)\n"
     ]
    }
   ],
   "source": [
    "print(train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#원핫 인코딩 된 모습.\n",
    "print(train_oh[0][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증데이터도 원핫이코딩 해주기.\n",
    "val_oh = keras.utils.to_categorical(val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 8)                 4072      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 4,081\n",
      "Trainable params: 4,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#SimpleRNN의 출력크기는 순환층의 뉴런개수와 동일한 8개의 출력을 확인할수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순환신경망 훈련하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 8s 22ms/step - loss: 0.7013 - accuracy: 0.5031 - val_loss: 0.6959 - val_accuracy: 0.5142\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6935 - accuracy: 0.5189 - val_loss: 0.6912 - val_accuracy: 0.5272\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6878 - accuracy: 0.5469 - val_loss: 0.6838 - val_accuracy: 0.5566\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.6771 - accuracy: 0.5806 - val_loss: 0.6657 - val_accuracy: 0.6124\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.6574 - accuracy: 0.6368 - val_loss: 0.6436 - val_accuracy: 0.6590\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.6320 - accuracy: 0.6790 - val_loss: 0.6256 - val_accuracy: 0.6818\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.6126 - accuracy: 0.7022 - val_loss: 0.6162 - val_accuracy: 0.6908\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.5979 - accuracy: 0.7189 - val_loss: 0.5967 - val_accuracy: 0.7090\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 0.5845 - accuracy: 0.7259 - val_loss: 0.5915 - val_accuracy: 0.7066\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.5737 - accuracy: 0.7369 - val_loss: 0.5828 - val_accuracy: 0.7150\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.5657 - accuracy: 0.7363 - val_loss: 0.5656 - val_accuracy: 0.7328\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5495 - accuracy: 0.7511 - val_loss: 0.5586 - val_accuracy: 0.7396\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5417 - accuracy: 0.7575 - val_loss: 0.5510 - val_accuracy: 0.7444\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5410 - accuracy: 0.7532 - val_loss: 0.5546 - val_accuracy: 0.7314\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5303 - accuracy: 0.7603 - val_loss: 0.5397 - val_accuracy: 0.7486\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5222 - accuracy: 0.7681 - val_loss: 0.5328 - val_accuracy: 0.7590\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5201 - accuracy: 0.7675 - val_loss: 0.5309 - val_accuracy: 0.7548\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.5076 - accuracy: 0.7767 - val_loss: 0.5228 - val_accuracy: 0.7594\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.4989 - accuracy: 0.7833 - val_loss: 0.5181 - val_accuracy: 0.7604\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.4966 - accuracy: 0.7809 - val_loss: 0.5169 - val_accuracy: 0.7612\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.4967 - accuracy: 0.7777 - val_loss: 0.5116 - val_accuracy: 0.7654\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.4808 - accuracy: 0.7933 - val_loss: 0.5082 - val_accuracy: 0.7656\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4863 - accuracy: 0.7816 - val_loss: 0.5076 - val_accuracy: 0.7654\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.4778 - accuracy: 0.7877 - val_loss: 0.5011 - val_accuracy: 0.7706\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.4749 - accuracy: 0.7916 - val_loss: 0.4995 - val_accuracy: 0.7700\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 7s 22ms/step - loss: 0.4721 - accuracy: 0.7915 - val_loss: 0.4961 - val_accuracy: 0.7700\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.4703 - accuracy: 0.7939 - val_loss: 0.4992 - val_accuracy: 0.7670\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.4669 - accuracy: 0.7969 - val_loss: 0.4985 - val_accuracy: 0.7650\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4620 - accuracy: 0.7979 - val_loss: 0.4890 - val_accuracy: 0.7732\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4617 - accuracy: 0.7958 - val_loss: 0.4872 - val_accuracy: 0.7752\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4565 - accuracy: 0.8030 - val_loss: 0.4867 - val_accuracy: 0.7758\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.4474 - accuracy: 0.8073 - val_loss: 0.4888 - val_accuracy: 0.7770\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.4427 - accuracy: 0.8080 - val_loss: 0.4883 - val_accuracy: 0.7724\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.4477 - accuracy: 0.8050 - val_loss: 0.4816 - val_accuracy: 0.7786\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.4454 - accuracy: 0.8045 - val_loss: 0.4814 - val_accuracy: 0.7762\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.4367 - accuracy: 0.8099 - val_loss: 0.4824 - val_accuracy: 0.7772\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.4341 - accuracy: 0.8156 - val_loss: 0.4794 - val_accuracy: 0.7802\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 10s 31ms/step - loss: 0.4295 - accuracy: 0.8144 - val_loss: 0.4770 - val_accuracy: 0.7806\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.4353 - accuracy: 0.8114 - val_loss: 0.4784 - val_accuracy: 0.7794\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.4299 - accuracy: 0.8137 - val_loss: 0.4774 - val_accuracy: 0.7798\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.4305 - accuracy: 0.8142 - val_loss: 0.4789 - val_accuracy: 0.7776\n"
     ]
    }
   ],
   "source": [
    "#기본 RMSprop의 학습률 0.001을 사용하지 않기 위해 학습률을 변경한다. 0.0001\n",
    "#epoch 횟수를 100으로 늘리고 배치크기는 64개로 설정한다.\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model.compile(optimizer = rmsprop, loss='binary_crossentropy', metrics = ['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5')\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                 restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data = (val_oh, val_target),\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가된 loss를 그래프로 그리기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-70c9b05d11b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000000 4000000000\n"
     ]
    }
   ],
   "source": [
    "#원핫인코딩으로 변환하면 단점이 입력데이터가 매우 커진다는 것이다.\n",
    "print(train_seq.nbytes, train_oh.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어임베딩(Word Embedding)을 사용하여 텍스트를 처리해주기.\n",
    "- 순환신경망에서 텍스트 처리에 자주 사용\n",
    "- 각 단어를 고정된 크기의 실수 벡터로 바꾸어 준다.\n",
    "- Embedding 클래스를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding 클래스를 추가한 순환신경망.\n",
    "#처음 500은 어휘 사전의 크기.\n",
    "#두번째는 임베딩 벡터의 크기.(원핫 인코딩에서는 500이었음.)\n",
    "\n",
    "model2 = keras.Sequential()\n",
    "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
    "model2.add(keras.layers.SimpleRNN(8))\n",
    "model2.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           8000      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 8)                 200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 8,209\n",
      "Trainable params: 8,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#(100,) 입력을 받아 (100,16)의 크기의 출력을 만든다.\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "313/313 [==============================] - 4s 9ms/step - loss: 0.6954 - accuracy: 0.5078 - val_loss: 0.6936 - val_accuracy: 0.5122\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6908 - accuracy: 0.5265 - val_loss: 0.6832 - val_accuracy: 0.5738\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.6759 - accuracy: 0.6157 - val_loss: 0.6603 - val_accuracy: 0.6684\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6502 - accuracy: 0.6875 - val_loss: 0.6367 - val_accuracy: 0.7020\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6254 - accuracy: 0.7263 - val_loss: 0.6139 - val_accuracy: 0.7338\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6024 - accuracy: 0.7437 - val_loss: 0.5945 - val_accuracy: 0.7352\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5815 - accuracy: 0.7574 - val_loss: 0.5772 - val_accuracy: 0.7412\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5641 - accuracy: 0.7657 - val_loss: 0.5595 - val_accuracy: 0.7624\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5440 - accuracy: 0.7782 - val_loss: 0.5452 - val_accuracy: 0.7702\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5268 - accuracy: 0.7868 - val_loss: 0.5345 - val_accuracy: 0.7634\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5137 - accuracy: 0.7913 - val_loss: 0.5216 - val_accuracy: 0.7694\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5008 - accuracy: 0.7941 - val_loss: 0.5117 - val_accuracy: 0.7718\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4889 - accuracy: 0.7964 - val_loss: 0.5056 - val_accuracy: 0.7732\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4790 - accuracy: 0.8014 - val_loss: 0.4959 - val_accuracy: 0.7788\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4707 - accuracy: 0.8020 - val_loss: 0.4887 - val_accuracy: 0.7840\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4630 - accuracy: 0.8042 - val_loss: 0.4837 - val_accuracy: 0.7836\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4499 - accuracy: 0.8089 - val_loss: 0.4820 - val_accuracy: 0.7828\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4474 - accuracy: 0.8097 - val_loss: 0.4760 - val_accuracy: 0.7836\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.4438 - accuracy: 0.8127 - val_loss: 0.4723 - val_accuracy: 0.7828\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.4417 - accuracy: 0.8075 - val_loss: 0.4730 - val_accuracy: 0.7814\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4305 - accuracy: 0.8183 - val_loss: 0.4705 - val_accuracy: 0.7832\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4331 - accuracy: 0.8161 - val_loss: 0.4700 - val_accuracy: 0.7840\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.4289 - accuracy: 0.8172 - val_loss: 0.4666 - val_accuracy: 0.7852\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 7s 24ms/step - loss: 0.4281 - accuracy: 0.8143 - val_loss: 0.4664 - val_accuracy: 0.7834\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.4134 - accuracy: 0.8247 - val_loss: 0.4666 - val_accuracy: 0.7830\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.4260 - accuracy: 0.8169 - val_loss: 0.4709 - val_accuracy: 0.7836\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.4147 - accuracy: 0.8216 - val_loss: 0.4650 - val_accuracy: 0.7824\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4164 - accuracy: 0.8191 - val_loss: 0.4645 - val_accuracy: 0.7864\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.4091 - accuracy: 0.8221 - val_loss: 0.4675 - val_accuracy: 0.7850\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.4131 - accuracy: 0.8198 - val_loss: 0.4662 - val_accuracy: 0.7870\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4070 - accuracy: 0.8256 - val_loss: 0.4694 - val_accuracy: 0.7818\n"
     ]
    }
   ],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5')\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
    "                                                 restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data = (val_seq, val_target),\n",
    "                    callbacks = [checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
